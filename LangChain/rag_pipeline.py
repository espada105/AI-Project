"""
RAG (Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸ ì‹¤ìŠµ ì½”ë“œ
í•¨ìˆ˜ ì¤‘ì‹¬ìœ¼ë¡œ ê° ë‹¨ê³„ë¥¼ êµ¬í˜„í•˜ì—¬ í”Œë¡œìš°ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±
"""

from dotenv import load_dotenv
import os
from typing import List, Dict, Tuple
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.schema import Document
from langchain.prompts import ChatPromptTemplate
from rank_bm25 import BM25Okapi
import numpy as np

load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")


# ============================================================================
# 1ë‹¨ê³„: ì¸ë±ì‹± (Indexing) - LSES íŒŒì´í”„ë¼ì¸
# ============================================================================

def load_documents() -> List[str]:
    """
    Load: ë¬¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    ì‹¤ìŠµìš© ì˜ˆì œ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‹¤ì œë¡œëŠ” PDF, TXT, DB ë“±ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    documents = [
        "LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. "
        "LangChainì€ ì²´ì¸(Chain) ê°œë…ì„ í†µí•´ ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        
        "RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ë¡œ, "
        "ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ë‹µë³€ í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. "
        "RAGëŠ” íŠ¹íˆ ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ì´ë‚˜ ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°ì— ìœ ìš©í•©ë‹ˆë‹¤.",
        
        "ë²¡í„° ì„ë² ë”©(Vector Embedding)ì€ í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. "
        "ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ë°°ì¹˜ë©ë‹ˆë‹¤. "
        "ì´ë¥¼ í†µí•´ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.",
        
        "BM25ëŠ” ì •ë³´ ê²€ìƒ‰ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìˆœìœ„ í•¨ìˆ˜ì…ë‹ˆë‹¤. "
        "TF-IDFë¥¼ ê°œì„ í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì— íš¨ê³¼ì ì…ë‹ˆë‹¤. "
        "BM25ëŠ” í¬ì†Œ ê²€ìƒ‰(Sparse Retrieval)ì˜ ëŒ€í‘œì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.",
        
        "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(Hybrid Search)ì€ BM25ì™€ Dense Embeddingì„ ê²°í•©í•œ ê²€ìƒ‰ ë°©ì‹ì…ë‹ˆë‹¤. "
        "RRF(Reciprocal Rank Fusion) ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë‘ ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœìœ„ë¥¼ í†µí•©í•©ë‹ˆë‹¤. "
        "ì´ë¥¼ í†µí•´ í‚¤ì›Œë“œ ë§¤ì¹­ê³¼ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì˜ ì¥ì ì„ ëª¨ë‘ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        
        "ReRankerëŠ” ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë” ì •í™•í•œ ëª¨ë¸ë¡œ ì¬ì •ë ¬í•˜ëŠ” ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤. "
        "Cross-Encoder ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ê³¼ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ ë” ì •ë°€í•˜ê²Œ í‰ê°€í•©ë‹ˆë‹¤. "
        "Lost in the Middle í˜„ìƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ìƒë‹¨ì— ë°°ì¹˜í•©ë‹ˆë‹¤.",
        
        "ChromaëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. "
        "ì„ë² ë”© ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. "
        "LangChainê³¼ í†µí•©ë˜ì–´ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        
        "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§(Prompt Engineering)ì€ LLMì—ê²Œ íš¨ê³¼ì ì¸ ì§€ì‹œë¥¼ ì œê³µí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. "
        "RAGì—ì„œëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µí•˜ê³ , "
        "LLMì—ê²Œ 'ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ë§Œ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ë¼'ëŠ” ì œì•½ì„ ë¶€ì—¬í•©ë‹ˆë‹¤."
    ]
    return documents


def split_documents(documents: List[str], chunk_size: int = 200, chunk_overlap: int = 50) -> List[Document]:
    """
    Split (Chunking): ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜
    - chunk_size: ê° ì²­í¬ì˜ ìµœëŒ€ í¬ê¸°
    - chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„ (ì˜ë¯¸ ì—°ì†ì„± ìœ ì§€)
    
    ì‹¤ìŠµ í¬ì¸íŠ¸: overlap í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ì—¬ ì˜ë¯¸ê°€ ëŠê¸°ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
    )
    
    # Document ê°ì²´ë¡œ ë³€í™˜
    docs = [Document(page_content=text) for text in documents]
    
    # ì²­í¬ë¡œ ë¶„í• 
    chunks = text_splitter.split_documents(docs)
    
    print(f"âœ… ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ â†’ {len(chunks)}ê°œ ì²­í¬")
    return chunks


def embed_and_store(chunks: List[Document], persist_directory: str = "./chroma_db") -> Chroma:
    """
    Embed & Store: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ VectorStoreì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    
    - Embedding: OpenAIì˜ text-embedding-ada-002 ëª¨ë¸ ì‚¬ìš©
    - Store: Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    """
    embeddings = OpenAIEmbeddings()
    
    # Chroma ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=persist_directory
    )
    
    print(f"âœ… ë²¡í„° ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
    return vectorstore


def create_bm25_retriever(chunks: List[Document]) -> BM25Retriever:
    """
    BM25 Retriever ìƒì„± í•¨ìˆ˜
    í‚¤ì›Œë“œ ê¸°ë°˜ í¬ì†Œ ê²€ìƒ‰ì„ ìœ„í•œ BM25 ì¸ë±ìŠ¤ ìƒì„±
    """
    retriever = BM25Retriever.from_documents(chunks)
    retriever.k = 5  # ìƒìœ„ 5ê°œ ê²°ê³¼ ë°˜í™˜
    
    print("âœ… BM25 Retriever ìƒì„± ì™„ë£Œ")
    return retriever


# ============================================================================
# 2ë‹¨ê³„: ê²€ìƒ‰ (Retrieve) - í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ + ReRank
# ============================================================================

def hybrid_search(
    query: str,
    vectorstore: Chroma,
    bm25_retriever: BM25Retriever,
    top_k: int = 10
) -> List[Document]:
    """
    Hybrid Search: BM25ì™€ Dense Embeddingì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    
    - Sparse Retrieval (BM25): í‚¤ì›Œë“œ ì¼ì¹˜ ê¸°ë°˜
    - Dense Retrieval (Embedding): ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜
    - RRF(Reciprocal Rank Fusion): ë‘ ê²°ê³¼ì˜ ìˆœìœ„ í†µí•©
    """
    # Ensemble Retriever ìƒì„± (RRF ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
    dense_retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
    
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, dense_retriever],
        weights=[0.4, 0.6]  # BM25 40%, Dense 60% ê°€ì¤‘ì¹˜
    )
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
    results = ensemble_retriever.get_relevant_documents(query)
    
    print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: '{query}' â†’ {len(results)}ê°œ ê²°ê³¼")
    return results


def rerank_documents(
    query: str,
    documents: List[Document],
    top_n: int = 3
) -> List[Document]:
    """
    ReRanking: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì •ë ¬í•˜ëŠ” í•¨ìˆ˜
    
    ì‹¤ìŠµìš© ê°„ë‹¨í•œ ReRanker êµ¬í˜„
    ì‹¤ì œë¡œëŠ” Cohere Rerank APIë‚˜ bge-reranker ê°™ì€ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    ì—¬ê¸°ì„œëŠ” ë¬¸ì„œ ê¸¸ì´ì™€ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨íˆ ì¬ì •ë ¬í•©ë‹ˆë‹¤.
    """
    def calculate_score(query: str, doc: Document) -> float:
        """ê°„ë‹¨í•œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ì‹¤ìŠµìš©)"""
        query_words = set(query.lower().split())
        doc_words = set(doc.page_content.lower().split())
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        keyword_score = len(query_words & doc_words) / len(query_words) if query_words else 0
        
        # ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™” (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë¬¸ì„œëŠ” í˜ë„í‹°)
        length_score = 1.0 / (1.0 + abs(len(doc.page_content) - 300) / 100)
        
        return keyword_score * 0.7 + length_score * 0.3
    
    # ê° ë¬¸ì„œì— ì ìˆ˜ ë¶€ì—¬
    scored_docs = [(doc, calculate_score(query, doc)) for doc in documents]
    
    # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    
    # ìƒìœ„ Nê°œ ë°˜í™˜
    reranked = [doc for doc, score in scored_docs[:top_n]]
    
    print(f"âœ… ReRanking ì™„ë£Œ: {len(documents)}ê°œ â†’ {top_n}ê°œë¡œ ì¶•ì†Œ")
    return reranked


# ============================================================================
# 3ë‹¨ê³„: ìƒì„± (Generation) - Context Injection + LLM Inference
# ============================================================================

def create_prompt_template() -> ChatPromptTemplate:
    """
    Prompt Engineering: RAGë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    
    í•µì‹¬ ì œì•½ ì‚¬í•­:
    - ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ë§Œ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€
    - ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ
    """
    template = """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œ ë‚´ìš©ë§Œ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
    
    return ChatPromptTemplate.from_template(template)


def generate_answer(
    query: str,
    context_docs: List[Document],
    llm: ChatOpenAI
) -> str:
    """
    Generation: ìµœì¢… ë‹µë³€ ìƒì„± í•¨ìˆ˜
    
    - Context Injection: ReRankëœ ë¬¸ì„œë“¤ì„ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…
    - LLM Inference: LLMì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•´ì„í•˜ì—¬ ë‹µë³€ ìƒì„±
    """
    # ì»¨í…ìŠ¤íŠ¸ ê²°í•©
    context = "\n\n".join([doc.page_content for doc in context_docs])
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt_template = create_prompt_template()
    prompt = prompt_template.format(context=context, question=query)
    
    # LLM í˜¸ì¶œ
    response = llm.invoke(prompt)
    
    return response.content


# ============================================================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸: ì „ì²´ RAG í”Œë¡œìš° ì‹¤í–‰
# ============================================================================

def run_rag_pipeline(query: str, use_reranker: bool = True):
    """
    ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
    
    í”Œë¡œìš°:
    1. ì¸ë±ì‹±: Load â†’ Split â†’ Embed & Store
    2. ê²€ìƒ‰: Hybrid Search (BM25 + Dense)
    3. ReRanking (ì„ íƒì )
    4. ìƒì„±: Context Injection â†’ LLM Inference
    """
    print("\n" + "="*60)
    print("ğŸš€ RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("="*60)
    
    # ========== 1ë‹¨ê³„: ì¸ë±ì‹± ==========
    print("\nğŸ“š [1ë‹¨ê³„] ì¸ë±ì‹± (Indexing)")
    print("-" * 60)
    
    # Load
    documents = load_documents()
    print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ")
    
    # Split
    chunks = split_documents(documents, chunk_size=200, chunk_overlap=50)
    
    # Embed & Store
    vectorstore = embed_and_store(chunks)
    
    # BM25 Retriever ìƒì„±
    bm25_retriever = create_bm25_retriever(chunks)
    
    # ========== 2ë‹¨ê³„: ê²€ìƒ‰ ==========
    print("\nğŸ” [2ë‹¨ê³„] ê²€ìƒ‰ (Retrieve)")
    print("-" * 60)
    
    # Hybrid Search
    retrieved_docs = hybrid_search(query, vectorstore, bm25_retriever, top_k=10)
    
    # ReRanking (ì„ íƒì )
    if use_reranker:
        retrieved_docs = rerank_documents(query, retrieved_docs, top_n=3)
    
    print(f"\nğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ({len(retrieved_docs)}ê°œ):")
    for i, doc in enumerate(retrieved_docs, 1):
        print(f"\n[{i}] {doc.page_content[:100]}...")
    
    # ========== 3ë‹¨ê³„: ìƒì„± ==========
    print("\nğŸ’¬ [3ë‹¨ê³„] ìƒì„± (Generation)")
    print("-" * 60)
    
    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7,
    )
    
    # ë‹µë³€ ìƒì„±
    answer = generate_answer(query, retrieved_docs, llm)
    
    print(f"\nâ“ ì§ˆë¬¸: {query}")
    print(f"\nâœ… ë‹µë³€:\n{answer}")
    
    print("\n" + "="*60)
    print("âœ¨ RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
    print("="*60)
    
    return answer, retrieved_docs


# ============================================================================
# ì‹¤ìŠµ ì‹¤í–‰ ì½”ë“œ
# ============================================================================

if __name__ == "__main__":
    # ì‹¤ìŠµ ì˜ˆì œ ì§ˆë¬¸ë“¤
    test_queries = [
        "RAGê°€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?",
        "ReRankerì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    ]
    
    # ì²« ë²ˆì§¸ ì§ˆë¬¸ìœ¼ë¡œ ì‹¤ìŠµ ì‹¤í–‰
    query = test_queries[0]
    
    # RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    answer, docs = run_rag_pipeline(query, use_reranker=True)
    
    print("\n" + "="*60)
    print("ğŸ’¡ ë‹¤ë¥¸ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:")
    print("   run_rag_pipeline('í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?')")
    print("="*60)
